-rw-r--r--   1 jun supergroup          0 2020-08-24 15:45 delay_counts_mos/_SUCCESS
-rw-r--r--   1 jun supergroup        171 2020-08-24 15:45 delay_counts_mos/arrival-r-00000
-rw-r--r--   1 jun supergroup        183 2020-08-24 15:45 delay_counts_mos/departure-r-00000
-rw-r--r--   1 jun supergroup          0 2020-08-24 15:45 delay_counts_mos/part-r-00000

[jun@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat delay_counts_mos/arrival-r-00000

2012,1  149036
2012,10 188527
2012,11 150009
2012,12 201482
2012,2  138349
2012,3  183040
2012,4  150954
2012,5  171789
2012,6  197091
2012,7  224610
2012,8  208734
2012,9  162147


나와있는 결과값이 100단위 1000단위라고 생각하고 여기서 sorting 을 해보자

출력 데이터의 키값 자체가 연도와 월이 합쳐진 문자열로 인식
보조정렬은 키의 값들을 그룹핑하고 그룹핑된 레코드에 순서를 부여하는 방식
보조정렬 구현순서
a. 기존 키의 값들을 조합한 복합키를 정의, 이때 키의 값중에서 어떤 키를 그룹핑 키로 사용할지 결정
b. 복합키의 레코드를 정렬하기 위한 비교기를 정의
c. 그룹핑 키를 파티셔닝할 파티셔너를 정의
d. 그룹핑 키를 정렬하기 위한 비교기를 정의

전부 자바를 이용해서 진행할 것들


기존 이용했던 항공 파일들을 3년치 6개월단위로 구해서 sorting 하기 위한 준비 파일로 사용
Notepad 를 이용해서 헤드 부분을 삭제하고 Download 에 넣어둔다.

새로 만들어놓은 파일들을
bin/hdfs dfs -put /mnt/share/download/csv* ariline_input 으로

만들어둔 폴더에 넣어둘 수 있다. 





리듀서에는 그룹핑 파티셔너와 그룹핑 comparator 에 의해 같은 연도로 그룹핑된 데이터가 전달된 상태
복합키 comparator 로 인해 그룹핑된 값은 월의 순서대로 오름차순으로 정렬되어 있음
하지만 리듀서 메서드에서 지연 횟수를 합산할 경우 데이터에 오류가 발생
예를 들어, 2008년도 항공 출발 지연 데이터를 처리할 경우 다음과 같은 결과가 나타남
2008 12 2647363
2008년도 12월만 출력되고 지연 횟수도 2008년도의 모든 지연 횟수가 합산되어 출력됨
이러한 현상이 나타나는 이유는 리듀서는 2008년 이라는 그룹키를 기준으로 연산을 수행하기 때문
월별로 지연 횟수를 계산하려면 복합키를 구분해서 처리하는 코드를 구현해야함
입력 데이터의 값에 해당하는 iterable 객체를 순회할 때 월에 해당하는 값을 bMonth 라는 변수에 백업
순회를 하면서 백업된 월과 현재 데이터의 월이 일치하지 않을때는 리듀서의 출력 데이터에 백업된 월의 지연 횟수를 출력
이때, 다음 순서에 잇는 월의 지연 횟수를 합산할 수 있게 지연 횟수 합계변수를 0으로 초기화







하둡 에코 시스템
하둡이 쓸만하니 하둡에 연계해 계속해서 새로운 시스템들이 나오기 시작함

pig
자바를 모르는 사람들이 쉽게 배울 수 있게 웹페이지로 맵리듀스 만드는 서비스 제공 - 지금은 사용안함

Hive
hdfs 에 들어있는 내용을 HiveQL 을 이용해서 SQL 비슷하게 사용해 사용할 수 있게 해줌 완벽한 SQL 은 아님
 
Flume
실시간 발생하는 log 를 스트림으로 받아서 hdfs 에 쏴줌

sqoop HDFS <--> DB 교환가능하게 해줌 , 요즘도 계속해서 쓰는 중



지금은 SQL 문으로 HDFS 를 다룰 수 있게 하는게 대세
SQL on Hadoop

우리는
Hive
sqoop
Tazo <- 고려대학교에서 만듬

3개를 해볼거다




Hive 먼저

mysql 가서 다운로드

MySQL Community (GPL) Downloads  들어가서

centos 는 yum
ubuntu 는 apt

각자 맞는 버전정보 다운받아서 폴더에서 원하는 곳에 cp 로 가져오기

vi /etc/yum.conf

gpqcheck=1 을 0 으로 바꿔준다.
다운로드한 파일을 yum 으로 설치해주기 위해서 필요한 작업
설치 끝나고 나서 다시 1 로 바꿔주면 됨

rpm -ivh mysql80-community-release-el7-3.noarch.rpm  깔아준다.




보통 mysql 한번 치고 접속하면 되는데,
리눅스의 시작프로그램으로 등록하고 싶다. 

systemctl enable mysqld


이 명령으로 시작하면서 바로 실행될 수 있게 해준다.

systemctl start mysqld

먼저 start 한번 해준거고

다음부터는 필요없이 자동으로 올라온다.

mysql root@localhost

처음 접속하기
하지만 password 가 필요하다고 나온다. 근데 우리는 패스워드 설정한적도 없고 모른다.

사실은, 자동으로 임시비밀번호가 생긴다.
cat /var/log/mysqld.log

이걸 보면,
localhost: gbufmzCnJ0+< 이런식으로 나옴

localhost 찾아서 뒤에 있는 비밀번호로 접속할 것.

mysql -u root -p

접속해서 비밀번호 넣어서 진행 < 접속 가능 >

비밀번호 바꾸기

alter user 'root'@'localhost' identified by 'tkdwns11@@aA' 

일단 어렵게 바꿔놓고 다시 쉽게 바꾸자



show variables like 'validate_password%';

비밀번호 규약 보기

set GLOBAL validate_password.length=4;
set GLOBAL validate_password.mixed_case_count=0;
set GLOBAL validate_password.policy=LOW;
set GLOBAL validate_password.special_char_count=0;

alter user 'root'@'localhost' identified by '1234';

이제 비밀번호 1234 로 바뀜



hive 에서 사용할 메타 데이터를 저장할 DB 를 만들어보자

create database hive_db default character set utf8;

 show databases; 입력

+--------------------+
| Database           |
+--------------------+
| hive_db            |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)




사용자 추가하기

하이브가 쓸 유저를 추가하고, 그 유저에게 하이브 권한을 다 줘야 한다.

하지만 처음에는 누군가에게 그 권한을 줄 권한이 없기 때문에
가장 먼저 하이브 root 에게 권한을 줘야 한다.

create user 'root'@'%' identified by '1234';
grant all privileges on *.* to 'root'@'%' with grant option;
flush privileges;


권한 root 에게 주고 하이브 유저 만들기
create user 'hiveuser'@'%' identified by '1234';


하이브 유저에게 권한 주기
grant all privileges on hive_db.* to hiveuser@'%';




quit 으로 나가기



다시 본래 계정으로 돌아와서

hive 깔기
https://hive.apache.org/
다운로드

2점대 3점대 버전별 정보 잘 보고 결정해서 다운로드 받을 것.

release 들어가서 아무대나 들어가서 2.3.7 링크주소 복사

wget http://apache.mirror.cdnetworks.com/hive/hive-2.3.7/apache-hive-2.3.7-bin.tar.gz
다운받기

tar zxvf apache-hive-2.3.7-bin.tar.gz

mv conf/hive-env.sh.template conf/hive-env.sh


vi conf/hive-env.sh

48번째 줄 # 해제하기

HADOOP_HOME=/home/jun/hadoop 으로 바꿔주기 (해당 주소)

저장하고 나와서

mv conf/hive-default.xml.template conf/hive-default.xml

conf/hive-default.xml

<property>
    <name>hive.metastore.local</name>
    <value>false</value>
    <descrip tion>location of default database for the warehouse</description>
</property>

hive.metastore 찾아서 근처에 추가하기




javax.jdo.option.ConnectionURL 찾아서
그 밑에

<value>jdbc:mysql://localhost:3306/hive_db?createDatabaseIfNotExist=true</value>

넣어주기


저장하기

javax.jdo.option.ConnectionDriverName 찾아서
밑에 

<value>com.mysql.jdbc.Driver</value>

넣어주기

저장하기




javax.jdo.option.ConnectionUserName 찾아서
밑에

<value>hiveuser</value>

넣어주기

저장하기



javax.jdo.option.ConnectionPassword 찾아서 ** 형식 잘 보고 그동안 했던거랑 비슷한거 찾기( * 한번 n 누르면 찾는다)
밑에

<value>1234</value>

넣어주고 저장하기



hive.cli.print.header 찾아서
밑에

<value>true</value>

넣어주고 저장하기













